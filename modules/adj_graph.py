# tensor([[0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
#    Author:  a101269
#    Date  :  2020/4/3
import torch
import torch.nn as nn
import os

# os.environ["CUDA_VISIBLE_DEVICES"] = "5"
#
# know_relaton = {'处所': ['机构', '建筑', '交通工具', '工具'], '机构': ['人'], '人': ['动物', '植物'],
#                 '生理': ['人', '心理', '事件'], '心理': ['人'], '药物': ['食物','工具'],'交通工具':['工具'],
#                 '属性': ['人', '动物', '植物', '建筑', '交通工具', '工具', '作品', '材料', '药物', '食物'],
#                 '材料': ['药物', '植物']}
know_relaton = {'处所': ['机构', '建筑', '交通工具', '工具'], '机构': ['人'], '人': ['动物', '植物'],
                '药物': ['食物','工具'],'交通工具':['处所','工具'],
                '材料': ['药物', '植物'],'建筑':['处所'],'食物':[ '药物'],
                '动物':['人','植物'],'植物':['动物','人','食物']}
class adj_graph(nn.Module):

    def __init__(self):
        super(adj_graph, self).__init__()

    def creat_graph(self,max_sent_len, vertex_num, knowledge_feature=None, know_relaton=None):
        # vertices
        # torch.manual_seed(123)
        # knowledge=torch.randn(4,5)
        # print(knowledge)
        # print(knowledge.triu(diagonal=1))  # diagonal斜线的，这里是偏移量  triu/tril
        # adj_matrix =torch.zeros([vertex_num, vertex_num]).cuda() if torch.cuda.is_available() else torch.zeros([vertex_num, vertex_num])
        adj_matrix = torch.cuda.LongTensor(vertex_num, vertex_num) if torch.cuda.is_available() else torch.LongTensor(vertex_num, vertex_num)
        tril_matrix = adj_matrix
        for i, col in enumerate(range(vertex_num, 0, -1)):
            adj_matrix = adj_matrix.triu(diagonal=col) + col
            tril_matrix = tril_matrix.tril(diagonal=-col) + col

        pad_zero = torch.cuda.LongTensor(vertex_num, vertex_num) if torch.cuda.is_available() else torch.LongTensor(vertex_num, vertex_num)
        pad_ones =pad_zero+1

        adj_matrix = torch.where(adj_matrix == 3, pad_ones, pad_zero)
        tril_matrix = torch.where(tril_matrix == 3, pad_ones, pad_zero)
        adj_matrix += tril_matrix
        # print(adj_matrix)
        adj_matrix[:, max_sent_len:] = 0
        adj_matrix[max_sent_len, max_sent_len - 1] = 0  # 句长矩阵，相邻词邻接
        # print(adj_matrix)
        for i, knowledge_id in enumerate(knowledge_feature):
            adj_matrix[i, max_sent_len + knowledge_id] = 1  # 词与对应知识标签邻接
        # print(adj_matrix)
        adj_matrix[max_sent_len:, max_sent_len:] = know_relaton
        # adj_matrix=torch.nn.functional.normalize( adj_matrix, p=2)
        # print(adj_matrix)
        return adj_matrix

    def creat_adj_matrix(self, max_sent_len, label_num, knowledge_feature=None, know_relaton=None):
        # batch_size = knowledge_feature.size()[0]
        vertex_num=max_sent_len + label_num
        for i, knowledge in enumerate(knowledge_feature):
            adj_matr = self.creat_graph(max_sent_len, vertex_num, knowledge_feature=knowledge, know_relaton=know_relaton)
            # if i>1:
            #     adj_matr=adj_matr.unsqueeze(0)
            if i == 0:
                batch_adj_matr = adj_matr.unsqueeze(0)
            else:
                # logger.warning('batch_adj_matr shape:%s',batch_adj_matr.shape)
                # logger.warning('adj_matr shape:%s',adj_matr.shape)
                batch_adj_matr = torch.cat((batch_adj_matr, adj_matr.unsqueeze(0)),0)
        del adj_matr
        return batch_adj_matr


def creat_sent_adj(vertex_num,none_label=None, knowledge_feature=None):
    adj_matrix = torch.cuda.LongTensor(vertex_num, vertex_num) if torch.cuda.is_available() else torch.LongTensor(
        vertex_num, vertex_num)
    adj_matrix*=0
    # adj_matrix+=0
    # print(adj_matrix)
    adj_num=0
    pre = none_label
    for i,label in enumerate(knowledge_feature):
        if i==0:
            if label==none_label:
                pre=none_label
            else:
                pre=label
                adj_num+=1
        elif label!=none_label:
            if label == pre:
                adj_num +=1
            elif label!=pre and pre == none_label:
                adj_num =1
                pre = label
            elif label!=pre and pre!=none_label and adj_num>1:
                for j in range(1,adj_num):
                    x = i - j
                    for k in (1,adj_num):
                        y = i-k
                        if x == y :
                            continue
                        adj_matrix[x, y] += pre
                        adj_matrix[y, x] += pre
                adj_num =1
                pre = label
            elif label != pre and pre != none_label and adj_num == 1:
                adj_matrix[i-1, i-1] += pre
        elif label==none_label:
            if adj_num>1:
                for j in range(1,adj_num):
                    x = i - j
                    for k in (1,adj_num):
                        y = i-k
                        if x == y:
                            continue
                        adj_matrix[x, y] += pre
                        adj_matrix[y, x] += pre
            elif adj_num == 1:
                adj_matrix[i - 1, i - 1] += pre
            adj_num =0
            pre=label
        if i==len(knowledge_feature)-1  and adj_num>1:
            for j in range(1, adj_num):
                x = i - j+1
                for k in (1, adj_num):
                    y = i - k+1
                    if x == y :
                       continue
                    # print(x, y)
                    # print(y, x)
                    adj_matrix[x, y] += pre
                    adj_matrix[y, x] += pre
        elif i==len(knowledge_feature)-1 and adj_num == 1:
            adj_matrix[i , i] += pre
    adj_mask = torch.gt(adj_matrix, 0)
    # print(batch_adj_mask)
    adj_matrix = adj_matrix.masked_fill(adj_mask, 1)
    return adj_matrix


def creat_adjoin_matrix(max_sent_len,none_label=None, knowledge_feature=None):
    # batch_size = knowledge_feature.size()[0]
    vertex_num=max_sent_len
    for i, knowledge in enumerate(knowledge_feature):
        adj_matr = creat_sent_adj(vertex_num,none_label=none_label, knowledge_feature=knowledge)
        if i == 0:
            batch_adj_matr = adj_matr.unsqueeze(0)
        else:
            # logger.warning('batch_adj_matr shape:%s',batch_adj_matr.shape)
            # logger.warning('adj_matr shape:%s',adj_matr.shape)
            batch_adj_matr = torch.cat((batch_adj_matr, adj_matr.unsqueeze(0)),0)
    # del adj_matr
    # print(batch_adj_matr)
    # batch_adj_matr=torch.nn.functional.normalize(batch_adj_matr.float(),dim=2, p=2)
    # batch_adj_mask = torch.gt(batch_adj_matr, 0)
    # print(batch_adj_mask)
    # batch_adj_matr = batch_adj_matr.masked_fill(batch_adj_mask, 1)
    return batch_adj_matr


if __name__ == '__main__':

    knowledge_feature = torch.tensor([[6,6, 10, 1, 1],[5, 5, 5, 2,10],[3, 3, 4, 4, 1]])
    knowledge_feature=knowledge_feature.cuda() if torch.cuda.is_available() else knowledge_feature

    # know_label = ['<PAD>', '<UNK>', '<ROOT>', '_', '搭配', '处所', '机构', '人', '动物', '植物', '抽象', '事件', '生理', '心理', '建筑',
    #               '工具','交通工具', '作品', '材料', '药物', '食物', '时间', '现象', '属性', '数目']
    # unit2id = {w: i for i, w in enumerate(know_label)}
    # print(unit2id)
    # label_num = len(know_label)
    # relation_matrix = torch.zeros([label_num, label_num])
    # for label, relation_labels in know_relaton.items():
    #     for rl in relation_labels:
    #         relation_matrix[unit2id[label], unit2id[rl]] = 1  # (5,6),(5,14)
    # print(relation_matrix)
    # creat_graph(4, label_num, knowledge_feature=knowledge_feature, know_relaton=relation_matrix)
    # knowledge_label = torch.range(0, 24, 1)
    # print(knowledge_label)
    adj=creat_adjoin_matrix(5, none_label=10, knowledge_feature=knowledge_feature)
    print(adj)
    # all_know_adj = torch.tensor([f for f in adj])
    # print(all_know_adj.shape)

    # adj_mask = torch.gt(adj, 0)
    # adj = adj.masked_fill(adj_mask, 1)

